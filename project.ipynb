{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project FDS\n",
    "\n",
    "Bone fracture detection on x-ray images using a CNN.\n",
    "\n",
    "Group members: Aiman Nadeem, Thale Krohn-Pettersen, Kirsten Nord.\n",
    "\n",
    "## Table of contents\n",
    "1. [Setup and data preprocessing](#setup)\n",
    "2. [Model definition](#model)\n",
    "3. [Training and validation](#training)\n",
    "4. [Analysis](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and data preprocessing <a name=\"setup\"></a>\n",
    "\n",
    "- Installing required packages\n",
    "- Importing necessary libraries\n",
    "- Downloading dataset and re-splitting the dataset into 80% training, 10% validation and 10% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\aiman\\anaconda3\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aiman\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\aiman\\anaconda3\\lib\\site-packages (from kagglehub) (23.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aiman\\appdata\\roaming\\python\\python311\\site-packages (from kagglehub) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aiman\\anaconda3\\lib\\site-packages (from kagglehub) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\aiman\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\aiman\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\aiman\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aiman\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aiman\\appdata\\roaming\\python\\python311\\site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aiman\\appdata\\roaming\\python\\python311\\site-packages (from requests->kagglehub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aiman\\anaconda3\\lib\\site-packages (from requests->kagglehub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aiman\\appdata\\roaming\\python\\python311\\site-packages (from requests->kagglehub) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\aiman\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    %pip install kagglehub scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to C:\\Users\\aiman\\.cache\\kagglehub\\datasets\\osamajalilhassan\\bone-fracture-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from Kaggle\n",
    "dataset_path = kagglehub.dataset_download(\"osamajalilhassan/bone-fracture-dataset\")\n",
    "print(f\"Dataset downloaded to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff'))\n",
    "\n",
    "def split_dataset(dataset_path, output_path, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    os.makedirs(output_path, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "    splits = ['training', 'testing']\n",
    "    categories = ['fractured', 'not_fractured']\n",
    "\n",
    "    for split in splits:\n",
    "        for category in categories:\n",
    "            class_dir = os.path.join(dataset_path, split, category)\n",
    "            \n",
    "            # Get all image files in the category\n",
    "            images = [f for f in os.listdir(class_dir) if is_image_file(f)]\n",
    "            random.shuffle(images)\n",
    "\n",
    "            # Calculate split sizes\n",
    "            train_end = int(len(images) * train_ratio)\n",
    "            val_end = train_end + int(len(images) * val_ratio)\n",
    "\n",
    "            train_images = images[:train_end]\n",
    "            val_images = images[train_end:val_end]\n",
    "            test_images = images[val_end:]\n",
    "\n",
    "            # Create subdirectories for train, val, test\n",
    "            for split_name, split_images in zip(\n",
    "                ['train', 'val', 'test'], [train_images, val_images, test_images]\n",
    "            ):\n",
    "                split_dir = os.path.join(output_path, split_name, category)\n",
    "                os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "                # Copy images to respective split folder\n",
    "                for image in split_images:\n",
    "                    src = os.path.join(class_dir, image)\n",
    "                    dst = os.path.join(split_dir, image)\n",
    "                    try:\n",
    "                        shutil.copy(src, dst)\n",
    "                    except PermissionError as e:\n",
    "                        print(f\"Skipping {src} due to permission error: {e}\")\n",
    "                        \n",
    "    print(f\"Dataset split into train, validation, and test sets at {output_path}\")\n",
    "\n",
    "# Perform the dataset split\n",
    "dataset_path = dataset_path + \"/BoneFractureDataset\"\n",
    "output_path = \"data\"\n",
    "split_dataset(dataset_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying dataset split...\n",
      "Number of images in train split:\n",
      "  Total images in train: 7570\n",
      "Number of images in val split:\n",
      "  Total images in val: 946\n",
      "Number of images in test split:\n",
      "  Total images in test: 947\n"
     ]
    }
   ],
   "source": [
    "def count_images_in_split(split_path):\n",
    "    # Count the number of image files in a given split directory (train, val, or test)\n",
    "    total_images = 0\n",
    "    for category in ['fractured', 'not_fractured']:\n",
    "        category_path = os.path.join(split_path, category)\n",
    "        if os.path.exists(category_path):\n",
    "            category_images = os.listdir(category_path)\n",
    "            total_images += len(category_images)\n",
    "    return total_images\n",
    "\n",
    "def verify_split(output_path):\n",
    "    # Verify the directory structure and the number of images in each split\n",
    "    splits = ['train', 'val', 'test']\n",
    "    print(\"Verifying dataset split...\")\n",
    "\n",
    "    for split in splits:\n",
    "        split_path = os.path.join(output_path, split)\n",
    "        if os.path.exists(split_path):\n",
    "            print(f\"Number of images in {split} split:\")\n",
    "            num_images = count_images_in_split(split_path)\n",
    "            print(f\"  Total images in {split}: {num_images}\")\n",
    "        else:\n",
    "            print(f\"  No {split} directory found!\")\n",
    "\n",
    "# Perform the verification\n",
    "verify_split(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition <a name=\"model\"></a>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation <a name=\"training\"></a>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis <a name=\"analysis\"></a>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
